{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11343386,"sourceType":"datasetVersion","datasetId":7097167}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torchvision import transforms\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T02:24:46.858272Z","iopub.execute_input":"2025-04-10T02:24:46.858572Z","iopub.status.idle":"2025-04-10T02:24:48.261014Z","shell.execute_reply.started":"2025-04-10T02:24:46.858551Z","shell.execute_reply":"2025-04-10T02:24:48.260102Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nclass CustomDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, resize=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.resize = resize\n        self.transform = transform\n        self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.png')])\n        self.mask_files = sorted([f for f in os.listdir(self.mask_dir) if f.endswith('.png')])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.image_files[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, self.resize)\n\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.resize(mask, self.resize)\n        mask = (mask > 127).astype(np.uint8)  # binary: white -> 1, black -> 0\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask'].unsqueeze(0)  # Add channel dimension: (1, H, W)\n\n        return image, mask\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T02:24:48.663779Z","iopub.execute_input":"2025-04-10T02:24:48.664248Z","iopub.status.idle":"2025-04-10T02:24:48.671058Z","shell.execute_reply.started":"2025-04-10T02:24:48.664221Z","shell.execute_reply":"2025-04-10T02:24:48.670032Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n\nclass PartialDecoder(nn.Module):\n    def __init__(self, in_channels_list, num_classes, reduced_channels=64):\n        super(PartialDecoder, self).__init__()\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(in_ch, reduced_channels, kernel_size=1)\n            for in_ch in in_channels_list\n        ])\n        self.final_conv = nn.Conv2d(reduced_channels * len(in_channels_list), num_classes, kernel_size=3, padding=1)\n\n    def forward(self, *features):\n        target_size = features[0].shape[2:]\n        outs = []\n        for conv, f in zip(self.conv_layers, features):\n            f_conv = conv(f)\n            f_up = F.interpolate(f_conv, size=target_size, mode='bilinear', align_corners=True)\n            outs.append(f_up)\n        concat = torch.cat(outs, dim=1)\n        seg_map = self.final_conv(concat)\n        return seg_map\n\n\nclass ReverseAttention(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(ReverseAttention, self).__init__()\n        self.seg_proj = nn.Conv2d(num_classes, in_channels, kernel_size=1)\n        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=3, padding=1)\n        self.out_conv = nn.Conv2d(num_classes, num_classes, kernel_size=3, padding=1)\n\n    def forward(self, high_level_feat, prev_seg_map, x_size):\n        seg_proj = self.seg_proj(prev_seg_map)\n        reverse_map = 1 - torch.sigmoid(seg_proj)\n        attention = high_level_feat * reverse_map\n        out_seg = self.conv(attention)\n        refined_seg = prev_seg_map + out_seg\n        side_output = self.out_conv(refined_seg)\n        side_output = F.interpolate(side_output, size=x_size, mode='bilinear', align_corners=True)\n        return refined_seg, side_output\n\n\nclass PraNet(nn.Module):\n    def __init__(self, in_channels=3, num_classes=1):\n        super(PraNet, self).__init__()\n        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n        self.encoder1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.encoder2 = resnet.layer1\n        self.encoder3 = resnet.layer2\n        self.encoder4 = resnet.layer3\n        self.encoder5 = resnet.layer4\n\n        self.pd = PartialDecoder(\n            in_channels_list=[256, 512, 1024, 2048],\n            num_classes=num_classes,\n            reduced_channels=64\n        )\n\n        self.ra5 = ReverseAttention(in_channels=2048, num_classes=num_classes)\n        self.ra4 = ReverseAttention(in_channels=1024, num_classes=num_classes)\n        self.ra3 = ReverseAttention(in_channels=512, num_classes=num_classes)\n\n    def forward(self, x):\n        x_size = x.shape[2:]\n\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(enc1)\n        enc3 = self.encoder3(enc2)\n        enc4 = self.encoder4(enc3)\n        enc5 = self.encoder5(enc4)\n\n        global_map = self.pd(enc2, enc3, enc4, enc5)\n\n        ra5_in = F.interpolate(global_map, size=enc5.shape[2:], mode='bilinear', align_corners=True)\n        refined5, side5 = self.ra5(enc5, ra5_in, x_size)\n\n        ra4_in = F.interpolate(refined5, size=enc4.shape[2:], mode='bilinear', align_corners=True)\n        refined4, side4 = self.ra4(enc4, ra4_in, x_size)\n\n        ra3_in = F.interpolate(refined4, size=enc3.shape[2:], mode='bilinear', align_corners=True)\n        refined3, side3 = self.ra3(enc3, ra3_in, x_size)\n\n        final_pred = side3\n\n        return final_pred, side3, side4, side5\n\n\nclass StructureLoss(nn.Module):\n    def __init__(self):\n        super(StructureLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, pred, mask):\n        pred = F.interpolate(pred, size=mask.shape[2:], mode='bilinear', align_corners=True)\n\n        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask.float(), kernel_size=31, stride=1, padding=15) - mask.float())\n        bce = self.bce(pred, mask.float())\n        bce = (weit * bce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n\n        pred_probs = torch.sigmoid(pred)\n        inter = (pred_probs * mask).sum(dim=(2, 3))\n        union = (pred_probs + mask).sum(dim=(2, 3))\n        iou = 1 - (inter + 1) / (union - inter + 1)\n\n        return (bce + iou).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T02:24:51.743949Z","iopub.execute_input":"2025-04-10T02:24:51.744273Z","iopub.status.idle":"2025-04-10T02:24:51.758416Z","shell.execute_reply.started":"2025-04-10T02:24:51.744248Z","shell.execute_reply":"2025-04-10T02:24:51.757454Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\n\n# ------------------- CONFIG -------------------\nH, W = 480, 480\nlr = 3e-4\nbatch_size = 16\nepochs = 300\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.4),\n    A.VerticalFlip(p=0.4),\n    A.RandomGamma(gamma_limit=(70, 130), p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# ------------------- LOAD DATA -------------------\ntrain_dataset = CustomDataset(\n    img_dir='/kaggle/input/datacv/drive-download-20250407T081859Z-001/TrainDataset/TrainDataset/image',\n    mask_dir='/kaggle/input/datacv/drive-download-20250407T081859Z-001/TrainDataset/TrainDataset/mask',\n    resize=(H, W),\n    transform=train_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# ------------------- TRAIN SETUP -------------------\nmodel = PraNet(num_classes=1).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/datacv/PraNet.pth\", map_location=device))\nloss_fn = StructureLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=5)\n\n# ------------------- TRAIN LOOP -------------------\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n\n    for images, masks in progress_bar:\n        images = images.to(device)\n        masks = masks.to(device).float()\n\n        optimizer.zero_grad()\n\n        final_pred, side3, side4, side5 = model(images)\n\n        loss_main = loss_fn(final_pred, masks)\n        loss_s3 = loss_fn(side3, masks)\n        loss_s4 = loss_fn(side4, masks)\n        loss_s5 = loss_fn(side5, masks)\n\n        total_loss = loss_main + loss_s3 + loss_s4 + loss_s5\n        total_loss.backward()\n        optimizer.step()\n\n        epoch_loss += total_loss.item()\n        progress_bar.set_postfix(loss=total_loss.item())\n\n    epoch_loss /= len(train_loader)\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n    scheduler.step(epoch_loss)\n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/model_weights.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndevice = 'cuda'\ndef infer(model, image_path, device, threshold=0.5):\n    model.eval()\n\n    # Load and preprocess image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    transformed = A.Compose([\n        A.Resize(480, 480),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])(image=image_rgb)\n\n    input_tensor = transformed['image'].unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(input_tensor)\n\n        if isinstance(outputs, tuple):\n            main_output = outputs[0]\n        else:\n            main_output = outputs\n\n        # Binary output: apply sigmoid and threshold\n        prob_mask = torch.sigmoid(main_output).squeeze().cpu().numpy()\n        binary_mask = (prob_mask > threshold).astype(np.uint8) * 255  # foreground=255, background=0\n\n    # Show results\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_rgb)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(binary_mask, cmap=\"gray\")\n    plt.title(\"Predicted Mask (Binary)\")\n    plt.axis(\"off\")\n    plt.show()\nmodel = PraNet(num_classes=1).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/datacv/PraNet.pth\", map_location=device))\nmodel.eval()\n\ninfer(model, \"/kaggle/input/datacv/drive-download-20250407T081859Z-001/TrainDataset/TrainDataset/image/105.png\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\n\n# === Dataset and Transforms ===\nval_transform = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# Replace with your actual CustomDataset definition\ntest_dataset = CustomDataset(\n    img_dir='/kaggle/input/comvsdataprime/TestDataset/TestDataset/Kvasir/images',\n    mask_dir='/kaggle/input/comvsdataprime/TestDataset/TestDataset/Kvasir/masks',\n    resize=(480, 480),\n    transform=val_transform,\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# === Load Model ===\nmodel = PraNet(num_classes=1)\nmodel.load_state_dict(torch.load(\n    \"/kaggle/input/comvsdataprime/PraNet.pth\",\n    map_location='cuda' if torch.cuda.is_available() else 'cpu'\n))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# === Metrics ===\ndef compute_iou(pred, mask, eps=1e-6):\n    pred = (pred > 0.5).float()\n    mask = (mask > 0.5).float()\n    intersection = (pred * mask).sum()\n    union = pred.sum() + mask.sum() - intersection\n    return (intersection + eps) / (union + eps)\n\ndef compute_dice(pred, mask, eps=1e-6):\n    pred = (pred > 0.5).float()\n    mask = (mask > 0.5).float()\n    intersection = (pred * mask).sum()\n    return (2 * intersection + eps) / (pred.sum() + mask.sum() + eps)\n\n# === Evaluation ===\nious, dices = [], []\n\nwith torch.no_grad():\n    for images, masks in tqdm(test_loader, desc=\"Evaluating\"):\n        images = images.to(device)\n        masks = masks.to(device).float()\n\n        outputs = model(images)\n        final_pred = outputs[0]  # only use the final refined output (side3)\n        probs = torch.sigmoid(final_pred)\n\n        for pred, true_mask in zip(probs, masks):\n            iou = compute_iou(pred, true_mask)\n            dice = compute_dice(pred, true_mask)\n            ious.append(iou.item())\n            dices.append(dice.item())\n\n# === Results ===\nmacro_iou = np.mean(ious)\nmacro_dice = np.mean(dices)\n\nprint(f\"ðŸ“Š Macro IoU: {macro_iou:.4f}\")\nprint(f\"ðŸ“Š Macro Dice: {macro_dice:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T02:34:17.968818Z","iopub.execute_input":"2025-04-10T02:34:17.969196Z","iopub.status.idle":"2025-04-10T02:34:20.626770Z","shell.execute_reply.started":"2025-04-10T02:34:17.969170Z","shell.execute_reply":"2025-04-10T02:34:20.625887Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-fc4e2ca006c2>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\nEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:02<00:00, 28.96it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Evaluation Results:\nðŸ”¹ Final (side3) â€” IoU: 0.7366, Dice: 0.8210\nðŸ”¸ Side 4         â€” IoU: 0.7223, Dice: 0.8115\nðŸ”¸ Side 5         â€” IoU: 0.7168, Dice: 0.8135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, ConcatDataset, Dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# === Dataset Class ===\nclass CustomDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, resize=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.resize = resize\n        self.transform = transform\n        self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.png')])\n        self.mask_files = sorted([f for f in os.listdir(self.mask_dir) if f.endswith('.png')])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.image_files[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, self.resize)\n\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.resize(mask, self.resize)\n        mask = (mask > 127).astype(np.uint8)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask'].unsqueeze(0)  # shape: (1, H, W)\n\n        return image, mask\n\n# === Transforms ===\nval_transform = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# === Dataset Setup ===\ndataset_names = ['CVC-300', 'CVC-ClinicDB', 'CVC-ColonDB', 'ETIS-LaribPolypDB', 'Kvasir']\nbase_path = '/kaggle/input/comvsdataprime/TestDataset/TestDataset'\n\nall_datasets = []\nfor name in dataset_names:\n    img_dir = os.path.join(base_path, name, 'images')\n    mask_dir = os.path.join(base_path, name, 'masks')\n    ds = CustomDataset(img_dir=img_dir, mask_dir=mask_dir, resize=(480, 480), transform=val_transform)\n    all_datasets.append(ds)\n\nfull_test_dataset = ConcatDataset(all_datasets)\ntest_loader = DataLoader(full_test_dataset, batch_size=1, shuffle=False)\n\n# === Model Setup ===\nmodel = PraNet(num_classes=1)\nmodel.load_state_dict(torch.load(\"/kaggle/input/comvsdataprime/PraNet.pth\", map_location='cuda' if torch.cuda.is_available() else 'cpu'))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# === Metrics ===\ndef compute_iou(pred, mask, eps=1e-6):\n    pred = (pred > 0.5).float()\n    mask = (mask > 0.5).float()\n    intersection = (pred * mask).sum()\n    union = pred.sum() + mask.sum() - intersection\n    return (intersection + eps) / (union + eps)\n\ndef compute_dice(pred, mask, eps=1e-6):\n    pred = (pred > 0.5).float()\n    mask = (mask > 0.5).float()\n    intersection = (pred * mask).sum()\n    return (2 * intersection + eps) / (pred.sum() + mask.sum() + eps)\n\n# === Evaluation Loop ===\nious, dices = [], []\n\nwith torch.no_grad():\n    for images, masks in tqdm(test_loader, desc=\"Evaluating All Datasets\"):\n        images = images.to(device)\n        masks = masks.to(device).float()\n\n        final_pred, _, _, _ = model(images)\n        probs = torch.sigmoid(final_pred)\n\n        for pred, true_mask in zip(probs, masks):\n            iou = compute_iou(pred, true_mask)\n            dice = compute_dice(pred, true_mask)\n            ious.append(iou.item())\n            dices.append(dice.item())\n\n# === Final Scores ===\nmacro_iou = np.mean(ious)\nmacro_dice = np.mean(dices)\n\nprint(f\"ðŸ“Š Macro IoU (All Datasets): {macro_iou:.4f}\")\nprint(f\"ðŸ“Š Macro Dice (All Datasets): {macro_dice:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T02:36:09.428557Z","iopub.execute_input":"2025-04-10T02:36:09.428915Z","iopub.status.idle":"2025-04-10T02:36:41.848586Z","shell.execute_reply.started":"2025-04-10T02:36:09.428891Z","shell.execute_reply":"2025-04-10T02:36:41.847647Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-790e778f52f5>:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/comvsdataprime/PraNet.pth\", map_location='cuda' if torch.cuda.is_available() else 'cpu'))\nEvaluating All Datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798/798 [00:31<00:00, 25.09it/s]","output_type":"stream"},{"name":"stdout","text":"ðŸ“Š Macro IoU (All Datasets): 0.5942\nðŸ“Š Macro Dice (All Datasets): 0.6715\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17}]}